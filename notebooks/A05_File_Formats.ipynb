{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File formats\n",
    "\n",
    "There are many different file formats in widespread use within data science. In this lecture, we will review common file formats and their trade-offs, and how to choose an appropriate file format. We will also review the mechanics of reading/parsing different file formats, and how to write to them.\n",
    "\n",
    "- [CSV](https://docs.python.org/3/library/csv.html)\n",
    "- [JSON](https://www.w3schools.com/js/js_json_intro.asp)\n",
    "- [XML](https://www.w3schools.com/xml/xml_whatis.asp)\n",
    "- [HDF5](https://support.hdfgroup.org/HDF5/Tutor/HDF5Intro.pdf)\n",
    "- [SQLite3](https://docs.python.org/3/library/sqlite3.html)\n",
    "\n",
    "These other formats may be touched on here but will be revisited when we look at big data and distributed computing.\n",
    "\n",
    "- [Parquet](https://parquet.apache.org/documentation/latest/)\n",
    "- [Avro](https://avro.apache.org/docs/current/)\n",
    "- [Arrow](https://arrow.apache.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard library packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import decimal\n",
    "import json\n",
    "import sqlite3\n",
    "import xml.etree.cElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd party packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --quiet faker json2xml fastparquet fastavro rec_avro pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json2xml import json2xml\n",
    "from json2xml.utils import readfromjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastavro \n",
    "from rec_avro import (to_rec_avro_destructive, \n",
    "                      from_rec_avro_destructive, \n",
    "                      rec_avro_schema)\n",
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fake profiles using `Faker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakes = [\n",
    "    Faker('zh_CN'), \n",
    "    Faker('ar_SA'), \n",
    "    Faker('en_US'), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "p = [0.3, 0.2, 0.5]\n",
    "np.random.seed(1)\n",
    "locales = np.random.choice(len(fakes), size=n, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = [fakes[locale].profile() for locale in locales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'job': 'Midwife',\n",
       "  'company': 'العجلان Ltd',\n",
       "  'ssn': '797-18-0626',\n",
       "  'residence': '94108 آل بن ظافر Track\\nالحكيرberg, NV 51805',\n",
       "  'current_location': (Decimal('45.2628705'), Decimal('96.088452')),\n",
       "  'blood_group': 'A-',\n",
       "  'website': ['https://www.lmhydb.com/', 'https://www.al.com/'],\n",
       "  'username': 'shyl42',\n",
       "  'name': 'راشد الجابر',\n",
       "  'sex': 'M',\n",
       "  'address': '252 شربتلي Field\\nNorth تاج الدّينborough, MO 36255',\n",
       "  'mail': 'bd-lkhlqldbg@hotmail.com',\n",
       "  'birthdate': datetime.date(2003, 12, 17)},\n",
       " {'job': 'Firefighter',\n",
       "  'company': 'Bishop-Franklin',\n",
       "  'ssn': '209-26-1758',\n",
       "  'residence': '29889 Thompson Ports\\nJeremyborough, MO 94138',\n",
       "  'current_location': (Decimal('-86.9495645'), Decimal('-16.874598')),\n",
       "  'blood_group': 'AB+',\n",
       "  'website': ['http://salazar.info/', 'https://www.haynes.info/'],\n",
       "  'username': 'craiggonzalez',\n",
       "  'name': 'Austin Cooley',\n",
       "  'sex': 'M',\n",
       "  'address': 'Unit 2453 Box 4102\\nDPO AA 66865',\n",
       "  'mail': 'candice52@hotmail.com',\n",
       "  'birthdate': datetime.date(1927, 10, 21)},\n",
       " {'job': '预定员',\n",
       "  'company': '双敏电子科技有限公司',\n",
       "  'ssn': '330624198404264763',\n",
       "  'residence': '安徽省敏市孝南程街y座 336159',\n",
       "  'current_location': (Decimal('73.0356085'), Decimal('-59.584268')),\n",
       "  'blood_group': 'AB+',\n",
       "  'website': ['http://chaoluo.cn/', 'http://39.cn/', 'https://xiulanduan.cn/'],\n",
       "  'username': 'leizheng',\n",
       "  'name': '傅兰英',\n",
       "  'sex': 'M',\n",
       "  'address': '澳门特别行政区长沙县孝南广州街C座 348407',\n",
       "  'mail': 'wei68@yahoo.com',\n",
       "  'birthdate': datetime.date(1945, 3, 27)}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make `pandas` data framee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job                                                           Midwife\n",
       "company                                                   العجلان Ltd\n",
       "ssn                                                       797-18-0626\n",
       "residence                94108 آل بن ظافر Track\\nالحكيرberg, NV 51805\n",
       "current_location                              (45.2628705, 96.088452)\n",
       "blood_group                                                        A-\n",
       "website                [https://www.lmhydb.com/, https://www.al.com/]\n",
       "username                                                       shyl42\n",
       "name                                                      راشد الجابر\n",
       "sex                                                                 M\n",
       "address             252 شربتلي Field\\nNorth تاج الدّينborough, MO ...\n",
       "mail                                         bd-lkhlqldbg@hotmail.com\n",
       "birthdate                                                  2003-12-17\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>company</th>\n",
       "      <th>ssn</th>\n",
       "      <th>residence</th>\n",
       "      <th>current_location</th>\n",
       "      <th>blood_group</th>\n",
       "      <th>website</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>address</th>\n",
       "      <th>mail</th>\n",
       "      <th>birthdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Midwife</td>\n",
       "      <td>العجلان Ltd</td>\n",
       "      <td>797-18-0626</td>\n",
       "      <td>94108 آل بن ظافر Track\\nالحكيرberg, NV 51805</td>\n",
       "      <td>(45.2628705, 96.088452)</td>\n",
       "      <td>A-</td>\n",
       "      <td>[https://www.lmhydb.com/, https://www.al.com/]</td>\n",
       "      <td>shyl42</td>\n",
       "      <td>راشد الجابر</td>\n",
       "      <td>M</td>\n",
       "      <td>252 شربتلي Field\\nNorth تاج الدّينborough, MO ...</td>\n",
       "      <td>bd-lkhlqldbg@hotmail.com</td>\n",
       "      <td>2003-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Firefighter</td>\n",
       "      <td>Bishop-Franklin</td>\n",
       "      <td>209-26-1758</td>\n",
       "      <td>29889 Thompson Ports\\nJeremyborough, MO 94138</td>\n",
       "      <td>(-86.9495645, -16.874598)</td>\n",
       "      <td>AB+</td>\n",
       "      <td>[http://salazar.info/, https://www.haynes.info/]</td>\n",
       "      <td>craiggonzalez</td>\n",
       "      <td>Austin Cooley</td>\n",
       "      <td>M</td>\n",
       "      <td>Unit 2453 Box 4102\\nDPO AA 66865</td>\n",
       "      <td>candice52@hotmail.com</td>\n",
       "      <td>1927-10-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>预定员</td>\n",
       "      <td>双敏电子科技有限公司</td>\n",
       "      <td>330624198404264763</td>\n",
       "      <td>安徽省敏市孝南程街y座 336159</td>\n",
       "      <td>(73.0356085, -59.584268)</td>\n",
       "      <td>AB+</td>\n",
       "      <td>[http://chaoluo.cn/, http://39.cn/, https://xi...</td>\n",
       "      <td>leizheng</td>\n",
       "      <td>傅兰英</td>\n",
       "      <td>M</td>\n",
       "      <td>澳门特别行政区长沙县孝南广州街C座 348407</td>\n",
       "      <td>wei68@yahoo.com</td>\n",
       "      <td>1945-03-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           job          company                 ssn  \\\n",
       "0      Midwife      العجلان Ltd         797-18-0626   \n",
       "1  Firefighter  Bishop-Franklin         209-26-1758   \n",
       "2          预定员       双敏电子科技有限公司  330624198404264763   \n",
       "\n",
       "                                       residence           current_location  \\\n",
       "0   94108 آل بن ظافر Track\\nالحكيرberg, NV 51805    (45.2628705, 96.088452)   \n",
       "1  29889 Thompson Ports\\nJeremyborough, MO 94138  (-86.9495645, -16.874598)   \n",
       "2                             安徽省敏市孝南程街y座 336159   (73.0356085, -59.584268)   \n",
       "\n",
       "  blood_group                                            website  \\\n",
       "0          A-     [https://www.lmhydb.com/, https://www.al.com/]   \n",
       "1         AB+   [http://salazar.info/, https://www.haynes.info/]   \n",
       "2         AB+  [http://chaoluo.cn/, http://39.cn/, https://xi...   \n",
       "\n",
       "        username           name sex  \\\n",
       "0         shyl42    راشد الجابر   M   \n",
       "1  craiggonzalez  Austin Cooley   M   \n",
       "2       leizheng            傅兰英   M   \n",
       "\n",
       "                                             address  \\\n",
       "0  252 شربتلي Field\\nNorth تاج الدّينborough, MO ...   \n",
       "1                   Unit 2453 Box 4102\\nDPO AA 66865   \n",
       "2                           澳门特别行政区长沙县孝南广州街C座 348407   \n",
       "\n",
       "                       mail   birthdate  \n",
       "0  bd-lkhlqldbg@hotmail.com  2003-12-17  \n",
       "1     candice52@hotmail.com  1927-10-21  \n",
       "2           wei68@yahoo.com  1945-03-27  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make comma delimited files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/profiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job,company,ssn,residence,current_location,blood_group,website,username,name,sex,address,mail,birthdate\r\n",
      "Midwife,العجلان Ltd,797-18-0626,\"94108 آل بن ظافر Track\r\n",
      "الحكيرberg, NV 5180"
     ]
    }
   ],
   "source": [
    "! head -c 200 data/profiles.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tab-delimited files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/profiles.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\tcompany\tssn\tresidence\tcurrent_location\tblood_group\twebsite\tusername\tname\tsex\taddress\tmail\tbirthdate\r\n",
      "Midwife\tالعجلان Ltd\t797-18-0626\t\"94108 آل بن ظافر Track\r\n",
      "الحكيرberg, NV 5180"
     ]
    }
   ],
   "source": [
    "! head -c 200 data/profiles.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make JSON files\n",
    "JavaScript Object Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(o):\n",
    "    if isinstance(o, datetime.datetime):\n",
    "        return o.__str__()\n",
    "    if isinstance(o, decimal.Decimal):\n",
    "        return o.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/profiles.json', 'w') as f:\n",
    "    json.dump(profiles , f, default=converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"job\": \"Midwife\", \"company\": \"\\u0627\\u0644\\u0639\\u062c\\u0644\\u0627\\u0646 Ltd\", \"ssn\": \"797-18-0626\", \"residence\": \"94108 \\u0622\\u0644 \\u0628\\u0646 \\u0638\\u0627\\u0641\\u0631 Track\\n\\u0627\\u0644\\u062d\\"
     ]
    }
   ],
   "source": [
    "! head -c 200 data/profiles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/profiles.xml', 'w') as f:\n",
    "    data = readfromjson('data/profiles.json')\n",
    "    f.write(json2xml.Json2xml({'employee': data}, wrapper=\"duke\").to_xml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\r\n",
      "<duke>\r\n",
      "\t<employee type=\"list\">\r\n",
      "\t\t<item type=\"dict\">\r\n",
      "\t\t\t<job type=\"str\">Midwife</job>\r\n",
      "\t\t\t<company type=\"str\">العجلان Ltd</company>\r\n",
      "\t\t\t<ssn type=\"str\">797-18-0626</ssn>\r\n",
      "\t\t\t"
     ]
    }
   ],
   "source": [
    "! head -c 200 data/profiles.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make AVRO files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = json.load(open('data/profiles.json'))\n",
    "avro_objects = [to_rec_avro_destructive(rec) for rec in ps]\n",
    "with open('data/profiles.avro', 'wb') as f_out:\n",
    "    fastavro.writer(f_out, fastavro.parse_schema(rec_avro_schema()), avro_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj\u0001\u0004\u0014avro.codec\bnull\u0016avro.schema�\u0005{\"__rec_avro_schema__\": true, \"type\": \"record\", \"name\": \"rec_avro.rec_object\", \"fields\": [{\"name\": \"_\", \"type\": [{\"type\": \"map\", \"values\": [\"null\", \"boolean\", \"int\","
     ]
    }
   ],
   "source": [
    "! head -c 200 data/profiles.avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Munge pandas data to be compratible with storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.birthdate = pd.to_datetime(df.birthdate)\n",
    "df = (\n",
    "    df.current_location.\n",
    "    apply(pd.Series).\n",
    "    merge(df, left_index=True, right_index=True).\n",
    "    drop('current_location', axis=1).\n",
    "    rename({0: 'location_x', 1: 'location_y'}, axis=1)\n",
    ")\n",
    "df['location_x'] = df['location_x'].astype('float')\n",
    "df['location_y'] = df['location_y'].astype('float')\n",
    "df.website = df.website.apply(lambda s: ','.join(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('data/profiles.h5', key='duke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�HDF\r",
      "\r\n",
      "\u001a\r\n",
      "\u0000\u0000\u0000\u0000\u0000\b\b\u0000\u0004\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000���������A\u0010\u0000\u0000\u0000\u0000\u0000��������\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000`\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0002\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0000\u0006\u0000\u0001\u0000\u0000\u0000\u0018\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0010\u0000\u0000\u0000\u0000\u0000 \u0003\u0000\u0000\u0000\u0000\u0000\u0000�\u0000\u0000\u0000\u0000\u0000\u0000\u0000TREE\u0000\u0000\u0001\u0000����������������\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000�\u0006\u0000\u0000\u0000\u0000\u0000\u0000\b\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000"
     ]
    }
   ],
   "source": [
    "! head -c 200 data/profiles.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Parquet files\n",
    "Also a schema (like AVRO)  \n",
    "Distributed\n",
    "Column format so pulling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastparquet.write('data/profiles.parq', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 data/profiles.parq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make SQLite3 database files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///data/profiles.sqlite', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('duke', con=engine, if_exists='replace', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -c 200 data/profiles.sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from different file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When the CSV file can be read as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When scrubbing of rows may be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open('data/profiles.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(len, rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows[1:], columns=rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab-delimited\n",
    "\n",
    "Same as CSV, just change separator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct reading into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/profiles.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row by row processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open('data/profiles.txt') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(len, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JSON is the most popular format for sharing information over the web. Most data retrieval APIs will return JSON.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/profiles.json') as f:\n",
    "    profiles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a REST API to retrieve JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/pokemon.json'):\n",
    "    ! curl -o data/pokemon.json https://pokeapi.co/api/v2/pokemon/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pokemon.json') as f:\n",
    "    pokemon = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon['abilities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten nested JSON and extract fields to `pandas`\n",
    "\n",
    "The [`json_normalize`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html) function is useful for extracting nested fields from a JSON document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(pokemon['abilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemons = [requests.get(f'https://pokeapi.co/api/v2/pokemon/{i}').json() \n",
    "            for i in range(20,25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(pokemons, ['moves', 'version_group_details'], ['name']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['name', 'move_learn_method.name',  'level_learned_at', 'version_group.name' ]\n",
    "pd.json_normalize(pokemons, ['moves', 'version_group_details'], ['name'])[cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.json_normalize(pokemons, 'abilities', ['name', 'order', 'weight', ['species', 'name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `explode` and `apply` methods are useful if you have nested structures within a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poke = pd.DataFrame(pokemons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poke.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `explode` to convert items in a list to separate rows, and `apply(pd.Series)` to convert items in a dictionary into separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_poke.abilities.explode().apply(pd.Series).reset_index(drop=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df1.ability.apply(pd.Series)).drop(columns = ['ability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('data/profiles.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET.dump(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for employee in root:\n",
    "    for elem in employee:\n",
    "        print(f'{elem.tag:>20}: {elem.text}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('.//')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in root.findall('.//company'):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5\n",
    "\n",
    "Like XML and JSON, HDF5 files store hierarchical data that can be annotated. The strong points of HDF5 are its ability to store large numerical data sets so that selective loading of parts of the data into memory for analysis is possible. HDF5 are also easy to use for people familiar with `numpy` and widely used in the scientific community.\n",
    "\n",
    "There are two popular libraries for working with HDF5. Pandas uses `pytables`, and the stored schema can be quite unintuitive, but that does not matter since we usually just use Pandas to read it back in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas and `tables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tables.open_file('data/profiles.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.root.duke.axis0[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.root.duke.axis1[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.root.duke.block0_items[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.root.duke.block0_values[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading into `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('data/profiles.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `h5py`\n",
    "\n",
    "For actually working directly with HDF5, I find `h5py` more intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/si.h5'\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "f = h5py.File(filename, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pendulum.datetime(2019, 8, 31)\n",
    "stop = start.add(days=3)\n",
    "for day in pendulum.period(start, stop):\n",
    "    g = f.create_group(day.format('ddd'))\n",
    "    g.attrs['date'] = day.format('LLL')\n",
    "    g.attrs['analyst'] = 'Mario'\n",
    "    for expt in range(3):\n",
    "        data = np.random.poisson(size=(100, 100))\n",
    "        ds = g.create_dataset(f'expt-{expt:02d}', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(filename, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f['Sat'].attrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Sat'].attrs['analyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Sat'].attrs['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(f['Sat'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Sat']['expt-01'][5:10, 5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Sat']['expt-01'][5:10, 5:10].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --quiet fastavro rec_avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out s\n",
    "fastavro --schema data/profiles.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = eval(s.replace('true', 'True'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/profiles.avro', 'rb') as f:\n",
    "    avro_reader = fastavro.reader(f, reader_schema=schema)\n",
    "    for record in avro_reader:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avro to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/profiles.avro', 'rb') as f:\n",
    "    avro_reader = fastavro.reader(f, reader_schema=schema)\n",
    "    for record in avro_reader:\n",
    "        print(from_rec_avro_destructive(record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --quiet fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq = fastparquet.ParquetFile('data/profiles.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parq.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading directly in `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/profiles.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "A relatinal databse isn't really a filetype, but SQLite3 stores data as a simple file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/profiles.sqlite')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"SELECT * FROM sqlite_master WHERE type='table'\")\n",
    "c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('SELECT * FROM duke')\n",
    "c.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
